# ğŸš€ Databricks 14 Days AI Challenge

This repository documents my **14-day hands-on learning journey on Databricks**, focused on building a strong foundation in **data engineering, analytics, and AI workflows** using **Apache Spark** and the **Lakehouse architecture**.

<img width="1835" height="288" alt="Image" src="https://github.com/user-attachments/assets/f40da4f2-6536-4977-9da3-7c24d2e5d8e7" />

The challenge follows a **daily structured format**, combining concepts, hands-on practice, and real-world use cases.  
This repository acts as a **learning log, portfolio reference, and public progress tracker**.



---

## ğŸ“Œ Table of Contents

- [About the Challenge](#-about-the-challenge)
- [What Youâ€™ll Learn](#-what-youll-learn)
- [Challenge Structure](#-challenge-structure)
- [Daily Progress Tracker](#-daily-progress-tracker)
- [Tech Stack](#-tech-stack)
- [Learning Objectives](#-learning-objectives)
- [Acknowledgements](#-acknowledgements)
- [Connect With Me](https://www.linkedin.com/in/sima-analyst/)

---

## ğŸ¢ About the Challenge

- ğŸ“Œ Organized by **Indian Data Club**
- ğŸ¤ In collaboration with **Codebasics**
- â­ Sponsored by **Databricks**

This initiative is designed to help learners move from **Databricks fundamentals** to **advanced analytics and AI-driven workflows** through structured, hands-on learning.

---

## ğŸ¯ What Youâ€™ll Learn

By completing this 14-day journey, I aim to gain practical exposure to the **end-to-end Databricks ecosystem**, including:

- **Databricks Platform & Workspace**
- **Lakehouse Architecture**
- **Apache Spark & PySpark**
- **Data Transformations & Window Functions**
- **Delta Lake & ACID Transactions**
- **Medallion Architecture (Bronzeâ€“Silverâ€“Gold)**
- **Workflow Automation & Jobs**
- **SQL Analytics & Performance Optimization**
- **MLflow & Experiment Tracking**
- **AI & Generative Analytics**

This ensures a **strong bridge between theory and real-world implementation**.

---

## ğŸ—ºï¸ Challenge Structure

The challenge is divided into **four progressive phases**:

### ğŸ”¹ PHASE 1: Foundation (Days 1â€“4)
- Databricks setup and workspace overview  
- Spark fundamentals  
- PySpark transformations  
- Basic analytics concepts  

### ğŸ”¹ PHASE 2: Data Engineering (Days 5â€“8)
- Delta Lake fundamentals and advanced features  
- Medallion architecture (Bronzeâ€“Silverâ€“Gold)  
- Workflow scheduling and automation  
- Governance basics  

### ğŸ”¹ PHASE 3: Advanced Analytics (Days 9â€“11)
- SQL analytics on Databricks  
- Query optimization and performance tuning  
- Statistical preparation for machine learning  

### ğŸ”¹ PHASE 4: AI & ML (Days 12â€“14)
- MLflow experiment tracking  
- Model comparison and evaluation  
- Introduction to AI and generative analytics  

By the end of the challenge, I aim to confidently handle **end-to-end Databricks workflows**, from raw data ingestion to AI-powered insights.

---

## ğŸ§  Daily Progress Tracker

| Day | Topic | Description | Status |
|----|------|------------|--------|
| âœ… Day 1 | Platform Setup & First Steps | Workspace, notebooks, and basic operations | Completed |
| âœ… Day 2 | Apache Spark Fundamentals | DataFrames, transformations, Spark basics | Completed |
| âœ… Day 3 | PySpark Transformations | Joins, window functions, feature engineering | Completed |
| âœ… Day 4 | Delta Lake Basics | Delta tables and ACID transactions | Completed |
| ğŸ”„ Day 5 | Delta Lake Advanced | Time travel and versioning | Planned |
| ğŸ”„ Day 6 | Medallion Architecture | Bronzeâ€“Silverâ€“Gold layers | Planned |
| ğŸ”„ Day 7 | Workflows & Jobs | Scheduling Databricks pipelines | Planned |
| ğŸ”„ Day 8 | Unity Catalog | Governance and access control | Planned |
| ğŸ”„ Day 9 | SQL Analytics | Analytical queries and insights | Planned |
| ğŸ”„ Day 10 | Performance Optimization | Caching and tuning | Planned |
| ğŸ”„ Day 11 | Stats & ML Prep | Preparing data for ML | Planned |
| ğŸ”„ Day 12 | MLflow Basics | Tracking ML experiments | Planned |
| ğŸ”„ Day 13 | Model Comparison | Evaluating model performance | Planned |
| ğŸ”„ Day 14 | AI & Generative Analytics | AI-powered analytics | Planned |

---

## ğŸ› ï¸ Tech Stack

This challenge is implemented using **Databricks Community Edition** and the following tools:

- **Apache Spark & PySpark** â€“ Distributed data processing  
- **Delta Lake** â€“ Reliable and versioned data lakes  
- **SQL** â€“ Analytics and querying  
- **MLflow** â€“ Experiment tracking and model management  
- **Python** â€“ Core programming language  

---

## ğŸ¯ Learning Objectives

- Understand modern data platform architecture  
- Build scalable data pipelines using Spark  
- Apply real-world data engineering patterns  
- Gain exposure to analytics and AI workflows  
- Learn by building and sharing progress publicly  

---

## ğŸ™ Acknowledgements

Special thanks to:
- **Databricks** for the platform  
- **Codebasics** for learning inspiration  
- **Indian Data Club** for organizing this challenge  

---

## ğŸ¤ Connect With Me

- ğŸ’¼ [LinkedIn](https://www.linkedin.com/in/sima-analyst/)
- ğŸ§‘â€ğŸ’» [GitHub](https://github.com/Sima-Mishra/Databricks-learning-journey/edit/main/README.md)
- ğŸ“º [YouTube](https://www.youtube.com/@datawithmishra)

---

â­ If you find this repository useful, feel free to star it and follow along with my learning journey!
